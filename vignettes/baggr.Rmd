---
title: "Aggregating evidence with baggr: Getting started with Average Treatment Effects"
author: "Witold Wiecek, Rachael Meager"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{baggr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
```


_Vignette under construction (duh)_

Baggr, pronounced "bagger" or "badger" and short for Bayesian Aggregator, is a package for aggregating evidence on causal effects measured in several separate and different instances. These instances may be different studies, groups, locations or "sites" however conceptualised. We refer to these separate pieces of evidence as "groups" for the remainder of this document. When each group is a study, the user is typically doing meta-analysis, but aggregation is not limited to this case.

One of the most basic objects of interest is the average treatment effect estimated using all the evidence from all the groups. Consider the case where the evidence in each study or group is generated by comparing the outcomes of treatment and control samples in a randomized experiment. We will ignore any covariate information at the individual or group level for now.

Consider some outcome of interest $y_{ik}$ such as consumption, income or health outcomes for a household or individual $i = 1,2,...N_k$ in study group $k = 1,2....K$. Let $Y_k$ denote the $N_k$-length vector of observed outcomes from group $k$. Denote the binary indicator of treatment status by $T_{ik}$, and denote by $T_k$ the $N_k$-length vector of all treatment status indicators from group $k$. Suppose that $y_{ik}$ varies randomly around its conditional mean $\mu_k + \tau_k T_i$. In this setting $\tau_k$ is the treatment effect in group $k$. The random variation in $y_{ik}$ may be the result of sampling variation or measurement error, as in the Rubin (1981) model, or it may be the result of unmodeled heterogeneity or uncertainty in outcomes for individuals within the group. Allow the variance of the outcome variable $y_{ik}$ to vary across sites, so $\sigma_{y_k}^2$ may differ across $k$. 

# Data inputs: reported effects or full individual-level data sets

For average effects aggregation, Baggr allows 3 types of data inputs. The user may supply 

(1) A set of estimated treatment effects $\{\hat{\tau_k}\}_{k=1}^{K}$ and their standard errors $\{\hat{se_k}\}_{k=1}^{K}$ from each study. This should be formatted as two vectors of length $K$, [RM Q: or a dataframe? surely it does not matter there] where $\hat{\tau_k}$ is the $k$th entry of the treatment effect vector and  $\hat{se_k}$ is the $k$th entry of the standard errors vector. 

(2) A set of both control group means and estimated treatment effects $\{\hat{\mu}_k,\hat{\tau}_k\}_{k=1}^{K}$, as well as the standard errors for both $\{\hat{se}_{\mu k}, \hat{se}_{\tau k}\}_{k=1}^{K}$, for each study site This should be formatted as four vectors of length $K$, [RM Q: or a dataframe? surely it does not matter there] analogous to the above.

(3) The full data sets from all the original studies $\{Y_k, T_k\}_{k=1}^{K}$. This should be formatted as three vectors of length $\sum_{k=1}^K N_{k}$. The $Y$ and $T$ vectors containing outcome and treatment should be generated by stacking the $K$ sites on top of one another. There should be a third vector of site indicators which can generally be constructed in R by first generating a $K$-length vector denoted Nk containing $(N_1, N_2..., N_K)$ and then using the R command rep(1:K, Nk).



```{r}
```


# Types of aggregation models in baggr

Baggr currently contains two different models suitable for estimating average treatment effects [ RM note: there is a potential confusion here. Our aggregation models always estimate an average effect of the group effects, but in the cases below, those group effects are also themselves the average treatment effects from the groups. For quantiles, we will estimate the average of the quantile effects across groups. I wonder if worth clarifying -- probably later, when the quantile models make it into this or another vignette.] Consider first the evidence aggregation model from Rubin (1981), discussed extensively in Chapter 5 of Bayesian Data Analysis by Gelman, Carlin, Rubin and Stern. This model is called "rubin" in baggr. The model consists of a hierarchical likelihood as follows:
\begin{equation}
\begin{aligned}
\hat{\tau_k} &\sim N(\tau_k, \hat{se_k}^2) \; \forall \; k \\
\tau_k &\sim N(\tau, \sigma_{\tau}^2) \; \forall \; k .
\end{aligned}
\label{rubin model}
\end{equation}


```{r}
```

# Running the Rubin Model in baggr


# Understanding and criticising the model

```{r}
```

# Reporting on baggr model

```{r}
```

